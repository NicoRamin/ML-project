{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import  AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BertModel, BertTokenizer\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset as FineTuneDataset\n",
    "\n",
    "# use GPU if available  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted the real data samples from the csv file.\n",
    "def read_csv(file):\n",
    "    real_texts = []\n",
    "    # Open the input file for reading\n",
    "    with open(file, mode='r', newline='', encoding=\"utf8\") as infile:\n",
    "        # Create a CSV reader object to read from the input file\n",
    "        csv_reader = csv.reader(infile)\n",
    "        # Get the header row\n",
    "        header = next(csv_reader)\n",
    "        # Find the index of the \"text\" column\n",
    "        text_column_index = header.index('text')\n",
    "        # Use islice to read only the first 10000 rows\n",
    "        for row in itertools.islice(csv_reader, 10000):\n",
    "            # append the text \n",
    "            real_texts.append(row[text_column_index])\n",
    "\n",
    "    return real_texts\n",
    "\n",
    "# call function to extract texts\n",
    "input_file = 'data/filtered_texts.csv'\n",
    "real_texts = read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify model name\n",
    "model_name = \"gpt2\" \n",
    "# Load the gpt-2 tokenizer\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load the gpt-2 model\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# set pad token as eos_token\n",
    "gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "\n",
    "gen_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Perspectives: 1st person, 3rd person\n",
    "2. Context: social Media, article, news, scientifc paper, story\n",
    "3. topic: politics, sport, research\n",
    "4. Time (present, past, future)\n",
    "5. Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the models generated output (it struggled a lot with spaces and special characters) \n",
    "def preprocess_string(sequence):\n",
    "    sequence = sequence.replace(\"\\n\", \" \")\n",
    "    sequence = sequence.replace(\"The text on the internet said the following:\", \"\")\n",
    "    sequence = sequence.replace(\"\\xa0\", \"\")\n",
    "    sequence = sequence.replace(\"'.'\", \"\")\n",
    "    sequence = sequence.replace(\"\\\\\", \"\")\n",
    "    sequence = sequence.replace(\"'\", \"\")\n",
    "    sequence = sequence.replace('\"', \"\")\n",
    "    sequence = sequence.lstrip(\" \")\n",
    "    sequence = sequence.lstrip(\".\")\n",
    "    sequence = sequence.replace(\"  \", \" \")\n",
    "    sequence = sequence.replace(\"   \", \" \")\n",
    "    \n",
    "    # Cut out everything after the last dot to avoid \n",
    "    sequence = sequence.rsplit('.',1)[0] +'.'\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def generator_generate(model):\n",
    "    # https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "    # According to OpenAi's estimation: 1 token corresponds to ~4 chars in english. The length of our dataset ranges from 118-299. A range from 80-200 tokens seems sufficient\n",
    "    random_length = random.randint(60, 180)\n",
    "\n",
    "    prompt = \"The text on the internet said the following:\"\n",
    "    input_ids = gen_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    # Generate tokens \n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        # set temperature high for variability in generations\n",
    "        temperature=0.8,\n",
    "        max_length=random_length,\n",
    "        pad_token_id=gen_tokenizer.eos_token_id\n",
    "    )\n",
    "    # decode tokens\n",
    "    gen_text = gen_tokenizer.batch_decode(gen_tokens)[0]\n",
    "    # preprocess text\n",
    "    preprocessed_gen_text = preprocess_string(gen_text)\n",
    "    return preprocessed_gen_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The government wants you to remember the facts. The people who fought for freedom, for justice and for equality will not accept any of this. There is no question about it. This is a matter for the people of Kerala. These things should not be tolerated. You cannot expect the Government of Tamil Nadu, which has the right to decide the issue, to accept and uphold any of these things in India. The Government of Kerala is responsible for all things related to the freedom of thought and expression of individuals in the State without any right to be held responsible or to be punished for violating them. The text has been released as a press release provided by the Union Ministry of Information and Broadcasting. The original version of the text can be found here.\n"
     ]
    }
   ],
   "source": [
    "# sample text generation\n",
    "text_test = generator_generate(gen_model)\n",
    "print(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Dataset including real and fake samples\n",
    "class FakeRealDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),  \n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),  \n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "max_length = 160  # Maximum length of input sequence\n",
    "\n",
    "# Initialize tokenizer\n",
    "disc_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "# Add classification head\n",
    "class BERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        # https://huggingface.co/transformers/v3.3.1/pretrained_models.html, hidden size of BERT base model is 768\n",
    "        # add prediction head of size 768x2 on model\n",
    "        self.fc = torch.nn.Linear(768, num_classes)  \n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        # Output layer for classification\n",
    "        self.fc = torch.nn.Linear(768, num_classes)  \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # apply pooling for better regulization \n",
    "        pooled_output = outputs.pooler_output\n",
    "        # add dropout for better regularization\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "    def predict(text, bert_model, tokenizer):\n",
    "        bert_model.eval()\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "        return predicted_class\n",
    "    \n",
    "\n",
    "def falsely_classified_as_real(predicted, labels, input_ids):\n",
    "    # real text = 0 \n",
    "    # fake text = 1\n",
    "    misclassified_texts = []\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] == 0 and labels[i] == 1:\n",
    "            current_text = disc_tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "            misclassified_texts.append(current_text)\n",
    "    return misclassified_texts\n",
    "    \n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Binary classification \n",
    "num_classes = 2  \n",
    "# initialise Discriminator\n",
    "disc_model = BERTClassifier(bert_model, num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(disc_model.parameters(), lr=1e-3)\n",
    "\n",
    "# # Freeze pre-trained weights to reduce inference time and avoid catastrophic forgetting\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label and combine fake and real texts\n",
    "def generate_texts(gen_model, num_generated_samples):\n",
    "    fake_texts = []\n",
    "    # generate fake texts using generator\n",
    "    for _ in tqdm(range(0, num_generated_samples)):\n",
    "        fake_texts.append(generator_generate(gen_model))\n",
    "\n",
    "    real_texts_random = random.sample(real_texts, num_generated_samples)\n",
    "\n",
    "    # Assigning labels\n",
    "    real_labels = [0] * len(real_texts_random)\n",
    "    fake_labels = [1] * len(fake_texts)\n",
    "\n",
    "\n",
    "    # Combine texts and labels\n",
    "    combined_texts = real_texts_random + fake_texts\n",
    "    combined_labels = real_labels + fake_labels\n",
    "\n",
    "    combined_data = list(zip(combined_texts, combined_labels))\n",
    "    # Shuffle the combined data\n",
    "    random.shuffle(combined_data)\n",
    "\n",
    "    gen_texts, gen_labels = zip(*combined_data) \n",
    "    gen_texts, gen_labels = list(gen_texts), list(gen_labels)\n",
    "\n",
    "    return gen_texts, gen_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train discriminator to classify texts\n",
    "def classify_texts(gen_texts, gen_labels, num_epochs, index):\n",
    "    # Create dataset instance\n",
    "    dataset = FakeRealDataset(gen_texts, gen_labels, disc_tokenizer, max_length)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = int(len(gen_texts)/4)\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    train_size = int(0.6 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders for training and validation sets\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    with open('training_results.txt', 'a') as epoch_file:\n",
    "        # iterate over epochs\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            disc_model.train()\n",
    "            # iterate over batch\n",
    "            for batch in train_dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = disc_model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute predictions and accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "            # Calculate accuracy for the epoch\n",
    "            accuracy = total_correct / total_samples\n",
    "            print(f'Epoch {epoch + 1}, Accuracy: {accuracy}')\n",
    "            # save model performance in file\n",
    "            epoch_file.write(f\"Iteration {index + 1}, Epoch {epoch + 1}, Loss: {loss.item()}, Accuracy: {accuracy}\\n\")\n",
    "\n",
    "\n",
    "    misclassified_examples = []\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # evaluate Discriminator  \n",
    "    disc_model.eval()  # Set the model to evaluation mode\n",
    "    # no weight update required\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = disc_model(input_ids, attention_mask)\n",
    "\n",
    "            # Compute predictions and accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            # save fake data that was classified as real for the fine tuning process\n",
    "            misclassified_examples.append(falsely_classified_as_real(predicted, labels, input_ids))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_accuracy = total_correct / total_samples\n",
    "    print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "    # save model performance in file\n",
    "    with open('test_results.txt', 'a') as test_file:\n",
    "        test_file.write(f\"Iteration {index + 1}, Test Accuracy: {test_accuracy}\\n\")\n",
    "\n",
    "    # Combined missclassified examples throughout epochs in list\n",
    "    new_misclassified_examples = list(itertools.chain(*misclassified_examples))\n",
    "    print(new_misclassified_examples)\n",
    "\n",
    "    # save discriminator\n",
    "    torch.save(disc_model.state_dict(), \"./discriminators/bert_discriminator_\" +  str(index+1) + \".pth\")\n",
    "\n",
    "\n",
    "\n",
    "    # filter out sequences that contain unnatural characters to prevent bad fine-tuning data\n",
    "    def filter_text_elements(text_elements):\n",
    "        # Define the characters to be filtered out\n",
    "        filter_chars = {'/', '\\\\', '_', '[', ']'}\n",
    "\n",
    "        # Filter the list\n",
    "        filtered_elements = [element for element in text_elements if not any(char in element for char in filter_chars)]\n",
    "        return filtered_elements\n",
    "\n",
    "    # call filter function \n",
    "    filtered_misclassified_examples = filter_text_elements(new_misclassified_examples)\n",
    "\n",
    "    # document missclassified examples\n",
    "    with open('missclassified_examples.txt', 'a') as file:\n",
    "        for iteration, sublist in enumerate(filtered_misclassified_examples):\n",
    "            # Convert sublist to a string\n",
    "            line = ' '.join(map(str, sublist))\n",
    "            # Create the line with the iteration number\n",
    "            line_to_write = f\"{str(iteration+1)}: {line}\"\n",
    "            # Write the string to the file, followed by a newline character\n",
    "            file.write(line_to_write + '\\n')\n",
    "\n",
    "   \n",
    "\n",
    "    return filtered_misclassified_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict class\n",
    "def predict(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=160).to(device)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function for fine tune data\n",
    "def tokenize_function(examples):\n",
    "    return gen_tokenizer(examples[\"text\"], truncation=True, padding='longest')\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=gen_tokenizer,\n",
    "    # For causal language modeling mlm should be set to False\n",
    "    mlm=False,  \n",
    ")\n",
    "\n",
    "# define fine tuning function \n",
    "def gen_fine_tune(new_misclassified_examples, index):\n",
    "    data_dict = {\"text\": new_misclassified_examples}\n",
    "    dataset = FineTuneDataset.from_dict(data_dict)\n",
    "    print(dataset)\n",
    "\n",
    "    # tokenize dataset\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Initialize the Huggingface Trainer\n",
    "    trainer = Trainer(\n",
    "        model=gen_model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_dataset,\n",
    "    )   \n",
    "\n",
    "    # train generator on fine-tuning data \n",
    "    trainer.train()\n",
    "\n",
    "    # save new generator \n",
    "    trainer.save_model(\"./generators/generator_\" + str(index+1))\n",
    "\n",
    "    # load new_generator\n",
    "    fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./generators/generator_\" + str(index+1))\n",
    "    fine_tuned_model.to(device)\n",
    "\n",
    "    return fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b660a86c124602bfa7fc3d7e1d7fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: GENERATOR: Text generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da59b30b35949dbbc4190523c2ccbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: DISCRIMINATOR: Text classification\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7823dfbd3dc4bc08a9b91280e8ae476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7755780220031738\n",
      "Epoch 1, Accuracy: 0.36666666666666664\n",
      "Epoch 2, Loss: 0.8118131756782532\n",
      "Epoch 2, Accuracy: 0.55\n",
      "Epoch 3, Loss: 0.9539410471916199\n",
      "Epoch 3, Accuracy: 0.5666666666666667\n",
      "Test Accuracy: 0.4\n",
      "[]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./discriminators_complex_new does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m generated_texts, generated_labels \u001b[38;5;241m=\u001b[39m generate_texts(new_gen_model, \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 2: DISCRIMINATOR: Text classification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m missclassified_examples \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 3: Fine Tuning generator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m new_gen_model \u001b[38;5;241m=\u001b[39m gen_fine_tune(missclassified_examples, index)\n",
      "Cell \u001b[1;32mIn[9], line 81\u001b[0m, in \u001b[0;36mclassify_texts\u001b[1;34m(gen_texts, gen_labels, num_epochs, index)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_misclassified_examples)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# save discriminator\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./discriminators_complex_new/bert_discriminator_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# filter out sequences that contain unnatural characters to prevent bad fine-tuning data\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_text_elements\u001b[39m(text_elements):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Define the characters to be filtered out\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ./discriminators_complex_new does not exist."
     ]
    }
   ],
   "source": [
    "new_gen_model = gen_model\n",
    "# iterate through the priorly defined steps\n",
    "for index in tqdm(range(0, 10)): \n",
    "    # generate texts and labels with 800 samples \n",
    "    print(\"Step 1: GENERATOR: Text generation\")\n",
    "    generated_texts, generated_labels = generate_texts(new_gen_model, 50)\n",
    "    print(\"Step 2: DISCRIMINATOR: Text classification\")\n",
    "    missclassified_examples = classify_texts(generated_texts, generated_labels, 3, index)\n",
    "    print(\"Step 3: Fine Tuning generator\")\n",
    "    new_gen_model = gen_fine_tune(missclassified_examples, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Example usage\n",
    "num_classes = 2  # fake or real\n",
    "model = BERTClassifier(bert_model, num_classes).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"./discriminators/bert_discriminator_{1}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict class\n",
    "def predict(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=160).to(device)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
